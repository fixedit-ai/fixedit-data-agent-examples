# This configuration file sets up a monitoring service that listens
# to any alarming detections and produces a metric that indicates
# if there currently is an alarm active or not.
#
# The way it works is essentially that when we get an alerting frame,
# we go into an alarming state. When there has been a long enough period
# of time since the last alerting frame, we go back into a normal state.
#
# To make sure that we trigger an alarm as quickly as possible, the first
# alerting frame that is received will trigger the alarming state. To avoid
# flickering the alarm state, we have a cool-off period that needs to pass
# before we can go back into a normal state.

# ---- Heartbeat ----
# This is a static heartbeat to trigger the inactivity monitor so we can
# disable the alarming state again.
[[inputs.exec]]
  commands = ["sh -c 'echo heartbeat value=1i'"]
  data_format = "influx"
  # This interval will set the cooldown period. If a whole hearbeat period
  # has passed without any alerting frame, then we will go back to normal
  # state. Theoretically, this just needs to be larger than the time
  # between two detections. In practice, two things play in:
  # 1. The object detector is not optimal, and might only get sporadic
  #    detections.
  # 2. This configuration is assuming timeley deliveries by Telegraf since
  #    we are essentially comparing to wall-time. This is not optimal and
  #    depending on load, some detections might arrive a bit late and some
  #    a bit quicker. Having a longer cool-off time is good to address this.
  # Note: The cool-off time is not exactly the same as the interval. One full
  # interval needs to pass without any alarms, meaning that in the most extreme
  # case, the cool-off time is almost two times the interval.
  interval = "5s"
  name_override = "alarming_state_heartbeat"

# ---- Starlark-based alerting frame counter ----
# This gets triggered by both the heartbeat and any alarming state metrics.
# This makes sure the code is run at every heartbeat even if there
# are no alarming state metrics which means that we can turn the state
# back to normal.
[[processors.starlark]]
  namepass = ["alarming_state_heartbeat", "alerting_frame_two"]
  source = '''
load("logging.star", "log")

# We initialize the state to keep it as a persistent
# state between calls. We can use it to store information
# such as the number of metrics since last heartbeat.
state = {
  # This variable is used to count how many "alerting_frame_two"
  # metrics have been received since the last
  # "alarming_state_heartbeat" metric.
  "alert_count_since_last_heartbeat": 0,

  # This flag makes sure that we send the alarming state metric
  # at most once per heartbeat to avoid overloading the downstream
  # consumers.
  "has_sent_alarming_state_metric": False
}

def send_alarm(time, active, count):
    alarming_state_metric = Metric("alerting_state_metric")
    alarming_state_metric.time = time
    alarming_state_metric.fields["active"] = active

    # The alert count is optional and only used at the end of an alarming period
    # since the early triggers are always sent after the first alarming frame.
    if count != None:
        alarming_state_metric.fields["alert_count"] = count

    return alarming_state_metric

def apply(metric):
    # Check how many alerting frames have been received since last heartbeat
    alert_count = state["alert_count_since_last_heartbeat"]

    # If we got an alerting frame, increment the counter and
    # send the alarming state metric if we have not already done so.
    if metric.name == "alerting_frame_two":
        state["alert_count_since_last_heartbeat"] += 1
        if not state["has_sent_alarming_state_metric"]:
            state["has_sent_alarming_state_metric"] = True
            return send_alarm(metric.time, True, None)
        return

    # Validate that the metric is a heartbeat
    if metric.name != "alarming_state_heartbeat":
        log.debug("Error: received metric with unexpected name: " + metric.name)
        return

    # Log the number of alerting frames received since last heartbeat
    log.debug("Alerting frames since last heartbeat: " + str(alert_count))

    # If no alarming frame has been received during the interval, then we should
    # go back to normal state.
    is_alarming = (alert_count > 0)

    # Always reset the counter to 0 at each heartbeat so we can start counting again.
    state["alert_count_since_last_heartbeat"] = 0

    # Always reset the has-sent flag - this means that we will resend the active
    # alarm each heartbeat interval. Not really needed, but could be in case
    # one message gets lost somewhere...
    state["has_sent_alarming_state_metric"] = False

    # Now send the end-of-period state update. This is needed to go back to normal
    # state. When is_alarming is True, we do not strictly need to send this metric
    # since it was already sent on the first alarming frame, but we do it anyways
    # so that we can report on the number of alerting frames in the period.
    return send_alarm(metric.time, is_alarming, alert_count)

'''

# # ---- File Output ----
# # Uncomment this section for debugging purposes only...
# [[outputs.file]]
#   # Only output alarming_state_metric metrics
#   namepass = ["alerting_state_metric"]

#   # Output to stdout
#   files = ["${HELPER_FILES_DIR}/alarms.txt"]

#   # Use JSON format for readable output
#   data_format = "json"